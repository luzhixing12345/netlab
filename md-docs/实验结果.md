
# 实验结果

本实验采用了两组虚拟环境在本机进行测试, 根据实验要求, 网络拓扑如下所示, 其中 client 的 IP 为 `202.100.10.2`, server 的 IP 为 `202.100.10.3`.

![20240101222638](https://raw.githubusercontent.com/learner-lu/picbed/master/20240101222638.png)

## 实验环境

### 环境一: docker

使用 docker-compose 搭建了一组基于 alpine 镜像的容器, 运行在 WSL2 内核上, 开启 sch_netem 以便于 tc 模拟, 并创建一个 bridge 网络进行通信, 网段为 202.100.10.0/24

在 monitor 中使用 SSH 分别连接到 client 和 server, 其中 client 的 IP 为 `202.100.10.2`, server 的 IP 为 `202.100.10.3`. monitor IP 为 `202.100.10.4`

![20240101222729](https://raw.githubusercontent.com/learner-lu/picbed/master/20240101222729.png)

虚拟网卡的数据包不经过任何队列,直接进行发送或接收

### 环境二: VMware WorkStation 虚拟机

使用两台虚拟机进行通信, 使用的操作系统为 Ubuntu22.04 (客户端) 和 Ubuntu18.04 (服务端), 均为 2 个 CPU, 每个 CPU 一个核心, 4 GB 内存

两台虚拟机均使用了fq_codel(Fair Queue Controlled Delay)调度器进行虚拟网卡配置, 该队列设置了最大包数目限制为10240个包,最大流的数量为1024,每个流可发送的最大数据包大小为1514字节, 目标延迟为5.0毫秒, 测量延迟的时间间隔为100.0ms, 队列内存限制为32Mb字节,并启用了显式拥塞通知(Explicit Congestion Notification)

### 基本配置信息

- 服务端发送线程数: 1024
- 服务端重发线程数: 64
- 服务端 ACK 接收线程数: 1088
- 客户端接收线程数: 1088
- 分块大小: 1024B
- 超过 RTT 的倍数(MAX\_RTT\_MULTIPLIER): 10
- 调整 RTT 的次数阈值(ADJUST\_RTT\_THRESHOLD): 1000

## 实验结果

tc(Traffic Control)是 Linux 系统中用于配置网络流量控制的命令行工具.在 tc 中 qdisc 队列调度器(Queue Discipline) 用于控制网络数据包的排队和调度,实现对网络带宽/延迟等性能参数的管理. qdisc 可以比较好的进行队列调度, 带宽控制和延迟控制, 提供了一种灵活的机制,使系统管理员能够根据具体需求和网络环境来调整和优化流量控制策略. 本实验中的网络丢包和延迟均使用 tc 模拟设置.

实验分别测试了在不同文件大小, 不同丢包率, 不同传输延迟下的传输时间. 文件大小为 0.1G 0.5G, 1G, 生成的文件内容为随机字符. 网络丢包率为 0\%, 5\%, 10\%, 延迟为 0ms, 50ms, 100ms. 

考虑到传输过程中存在一定程度的网络波动,因此每组测试五次, 传输时间取平均值后的结果如下.测试结果显示,所有实验均完成文件传输,并且通过 md5 校验和检测,成功在系统中实现了server与client之间的无误通信,该部分的实验已经达到了实验要求.

实验结果发现传输时间与文件大小程线性关系变化.在文件体积较小时丢包和延迟的影响效果均不明显,文件体积较大后额外增加的传输时间和丢包率的值也有比较好的线性映射关系.

| 文件大小/G | 丢包率/% | 传输时间/s |
| :--------: | :------: | :--------- |
|    0.1     |    0     | 14.83      |
|    0.1     |    5     | 14.31      |
|    0.1     |    10    | 14.15      |
|    0.5     |    0     | 57.62      |
|    0.5     |    5     | 57.99      |
|    0.5     |    10    | 62.03      |
|     1      |    0     | 123.09     |
|     1      |    5     | 125.78     |
|     1      |    10    | 141.11     |

| 文件大小/G | 延迟/ms | 传输时间/s |
| :--------: | :-----: | :--------: |
|    0.1     |    0    |   12.27    |
|    0.1     |   50    |   13.62    |
|    0.1     |   100   |   17.67    |
|    0.5     |    0    |   51.53    |
|    0.5     |   50    |   61.57    |
|    0.5     |   100   |   77.86    |
|     1      |    0    |   110.13   |
|     1      |   50    |   124.89   |
|     1      |   100   |   153.42   |

通过两个表格展示了在不同丢包率和延迟条件下的传输性能.实验结果发现传输时间与文件大小程线性关系变化.在文件体积较小时丢包和延迟的影响效果均不明显,文件体积较大后额外增加的传输时间和丢包率的值也有比较好的线性映射关系.丢包率和延迟对于传输时间的影响在不同文件大小下表现出一定的规律性.延迟实验结果显示,增加延迟会导致传输时间逐渐增加,而大文件在受到延迟的影响更为显著.

## 实验结论

通过本次实验,本实验成功地重新设计了基于UDP的可靠传输协议,引入了TCP三次握手/数据交换/确认报文回复/选择重传等机制,并结合多线程并行和拥塞控制,以提高数据传输的可靠性和效率.

-  协议设计: 本实验重新设计了应用层的协议,结合了TCP的可靠传输机制,但保留了UDP的轻量级和无连接特性.
-  三次握手机制: 引入TCP的三次握手确保了通信的可靠建立,防止了不必要的数据传输.
-  选择性重传: 通过选择性重传机制,成功应对了数据包丢失的情况,提高了数据传输的可靠性.
-  多线程并行: 引入多线程并行机制,充分利用系统资源,提高了整体传输性能.
-  拥塞控制: 通过引入拥塞控制机制,避免了网络拥塞导致的性能下降.

实验中设计的协议在数据的可靠交换/确认报文回复等方面取得了令人满意的结果,成功解决了丢包问题.同时协议保持了UDP的轻量级和无连接特性,同时通过重新设计的机制实现了可靠传输,增加了协议的灵活性和适用性.

在本次实验中,通过深入研究网络传输机制,我深刻理解了可靠传输的重要性以及在UDP协议下的挑战.在未来的研究中,可以进一步考虑对协议的优化和适用性扩展,以适应更复杂的网络环境.同时,也要注意在实际应用中平衡可靠性和性能,根据具体场景选择合适的协议和机制.本次实验让我对网络通信领域有了更深层次的认识,对于解决实际问题提供了有益的经验.